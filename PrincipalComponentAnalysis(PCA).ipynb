{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why PCA ?\n",
    "------------\n",
    "Feature Engineering is the process of reducing the dimensionality by creating new features from the given features.\n",
    "Feature Selection is the process of reducing the dimensionality by selecting a subset of important features from the given features.\n",
    "\n",
    "Examples of Feature Engineering Techniques: PCA, T-SNE.\n",
    "Examples of Feature Selection Techniques: Forward Feature Selection, Backward Eliminatio\n",
    "\n",
    "How PCA is performed ?\n",
    "---------------\n",
    "PCA is done by reducing the n-dimension data to n' dimension for easy visualization for features/attributes\n",
    "Let us assume we have around 1000 features in the given dataset 'A' and we want to reduce the dimensionality.\n",
    "So we apply PCA and try to pick those components that preserve as much maximum variance of the initially given data 'A' as possible.\n",
    "Affter applying PCA, let us assume the maximum amount of variance(ie., 100%) is explained by 200 features from the newly formed features(ie., features obtained after the rotation). Then we reduce the dimensionality to these new 200 features discarding the remaining features.\n",
    "\n",
    "\" features obtained after the rotation\" what does this imply? Can you please elaborate?\n",
    "\n",
    "These are the new axes that could retain maximum information. Out of these newly formed axes, we pick only those top 'K' features that retain maximum information and use this dimensionality reduced data for model building\n",
    "\n",
    "PCA name itself signifies what it does. It identifies the main features which can distinguish between the classes.\n",
    "Before going for PCA, we should do column standardization. CS can help us bring the features under unifom metric and we can observe the variance of each feature easily.\n",
    "\n",
    "From Geometrical interpretaion, if we are unable to eliminate one feature from scattered plate by analysing Variance, we can rotate the axis of features by some angle and align the rotated line on to the data points. This way we can analyse variance more lucidly and eliminate one feature\n",
    "\n",
    "Mathematical Intutiton:\n",
    "--------------\n",
    "If we have a 2D space.\n",
    "Feature1 = weight (lets say)\n",
    "Feature2 = height (lets say)\n",
    "\n",
    "As a first step we would want to apply standardization for each feature first.\n",
    "After applying the transformation we have a 2D data set centered as the origin with std of 1.\n",
    "This implies that each of our variables would have the exact same spread (and therefore we would not know at this point which feature is the better one to use)\n",
    "\n",
    "We will therefore always need to do an axis transformation (find unit vector) in order for the axis to lineup with features that have the highest spread?\n",
    "the unit vector = just a new feature created (with its data observations the projected values)\n",
    "\n",
    "So for PCA it is mandatory to normalize our dataset because if the features have different scales then our principal components will be more close to the high scaled features.Nextly since we have standardised data the mean = 0 and std = 1 in case of PCA.So the eigen vectors we compute has norm = 1.\n",
    "\n",
    "we have to find the features which minimizes the variance, we will find an axis and project all the points on them and now as the points will now lie on that straight axis(as we have projected all the points on that axis). So we have to find such axis which minimizes the variance along its direction(F1')\n",
    "\n",
    "So for finding variance we will need the mean (mean vector)along that axis and the components of points along that axis. Now here standardization comes into picture. As after standardization the mean becomes zero, so all we have to do is minimize the sum of projection of all the points on that axis. In this way we find all the principal axes. Now please go through the video again if anything is still unclear. Now you will understand it better.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective of PCA is to reduce the dimensionality by preserving maximum variance/spread of the data. Here in MNIST each data point is given in the form of an image with 784 pixels. So as per featurization, we are converting those 784 pixels into 784 dimensions and then our job is to classify the number each data point represents. So after converting each data point into 784 dimensional point, as there is redundant data we are trying to reduce the dimensionality such that top newly formed axes represent the dimensions with huge variance.\n",
    "For example, let us consider the example of the data about cricket players of tournament from the beginning till now.\n",
    "Let us assume the dimensions we have are\n",
    "AGE, NUMBER OF MATCHES PLAYED, NUMBER OF 100s scored, NUMBER OF 50s scored, NUMBER OF WICKETS TAKEN, etc.\n",
    "In this case, if we have around data points of 1000 players, then if the range of a feature is very small, then we say that feature has low variance.\n",
    "If number of 100s scored is in between 100-110 and if age lies in between 25-28, number of 50s in between 300-500, number of wickets taken in between 50-200 then we can say that the feature 'Age' has least variance when compared to the other features as it takes smaller set of possible values. The feature 'number of 50s' is said to have higher variance. In this case, after applying PCA, the feature 'number of 50s' will be the top feature (highly important) and 'Age' is the least important feature. The feature 'number of 50s' explains maximum variance when compared to the other features individually. So whenever we want to consider only those features that could explain upto 90% or 95% pf variance, then we need to take the cumulative sum of explained variance of various number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
